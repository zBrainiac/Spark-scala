import org.apache.log4j.{Level, Logger}
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions.when
import org.apache.spark.sql.types.StringType

object CobolSchemaValidation {

  def main(args: Array[String]): Unit = {
    // Switch logging level to WARN
    Logger.getLogger("org").setLevel(Level.WARN)
    Logger.getLogger("akka").setLevel(Level.WARN)

    val sparkSession = SparkSession
      .builder()
      .appName("Spark-Cobol Companies example")
      .master("local")
      .getOrCreate()
    val CompaniesDF = sparkSession.read
      .format("cobol")
      .option("copybook", "data/companies_copybook.cpy")
      .option("is_record_sequence", "true")                // This file is a sequence of records with 4 byte record headers
     // .option("generate_record_id", "true")                // Generates File_Id and Record_Id fields for line order dependent data
      .option("schema_retention_policy", "collapse_root")  // Collapses the root group returning it's field on the top level of the schema
      .option("segment_field", "SEGMENT_ID")               // The SEGMENT_ID field contains IDs of segments
      .option("segment_id_level0", "C")                    // Root segment is equals "C" [company]. All other segments are it's childern
      .option("segment_id_prefix", "ID")                   // This prefix will be added to each record autogenerated ID
      .load("data/companies_data")                  // Location of data file(s)

    CompaniesDF.printSchema()
    println(CompaniesDF.count)
    CompaniesDF.show(50, truncate = false)

    val CompaniesFilterDF = CompaniesDF
      .filter("SEGMENT_ID == 'C'")
      .select("Seg_Id0", "COMPANY_ID", "STATIC_DETAILS.COMPANY_NAME", "STATIC_DETAILS.ADDRESS")
    CompaniesFilterDF.show(50, false)

    val ContactsDF = CompaniesDF
      .filter("SEGMENT_ID == 'P'")
      .select("Seg_Id0", "COMPANY_ID", "CONTACTS.CONTACT_PERSON", "CONTACTS.PHONE_NUMBER")

    ContactsDF.printSchema()
    println(ContactsDF.count)
    ContactsDF.show(50, truncate = false)

    val JoinedDF = CompaniesFilterDF.join(ContactsDF, "Seg_Id0")

    JoinedDF.printSchema
    println(JoinedDF.count)
    JoinedDF.orderBy("Seg_Id0").show(50, truncate = false)
  }
}
